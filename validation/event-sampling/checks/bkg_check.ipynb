{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background checks\n",
    "\n",
    "This nootebook tests the sampling of the background. We simulate only the background and we then fit it, leaving the tilt and normalization parameters free to vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "import click\n",
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.units as u\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "# from gammapy.cube import (\n",
    "#     MapDatasetEventSampler,\n",
    "# )\n",
    "from gammapy.data import GTI, Observation, EventList\n",
    "from gammapy.datasets import MapDataset\n",
    "from gammapy.detect import LiMaMapEstimator as lima\n",
    "from gammapy.maps import MapAxis, WcsGeom, Map\n",
    "from gammapy.irf import EnergyDispersion2D, load_cta_irfs\n",
    "from gammapy.makers import MapDatasetMaker\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import Models\n",
    "from gammapy.utils.table import table_from_row_data\n",
    "from regions import CircleSkyRegion\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "import gammapy\n",
    "from gammapy.data import EventList\n",
    "from gammapy.maps import MapCoord\n",
    "from gammapy.modeling.models import BackgroundModel, ConstantTemporalModel\n",
    "from gammapy.utils.random import get_random_state\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDatasetEventSampler:\n",
    "    \"\"\"Sample events from a map dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random_state : {int, 'random-seed', 'global-rng', `~numpy.random.RandomState`}\n",
    "        Defines random number generator initialisation.\n",
    "        Passed to `~gammapy.utils.random.get_random_state`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_state=\"random-seed\"):\n",
    "        self.random_state = get_random_state(random_state)\n",
    "\n",
    "    def _sample_coord_time(self, npred, temporal_model, gti):\n",
    "        n_events = self.random_state.poisson(np.sum(npred.data))\n",
    "\n",
    "        coords = npred.sample_coord(n_events=n_events, random_state=self.random_state)\n",
    "\n",
    "        table = Table()\n",
    "        try:\n",
    "            energy = coords[\"energy_true\"]\n",
    "        except KeyError:\n",
    "            energy = coords[\"energy\"]\n",
    "\n",
    "        table[\"ENERGY_TRUE\"] = energy\n",
    "        table[\"RA_TRUE\"] = coords.skycoord.icrs.ra.to(\"deg\")\n",
    "        table[\"DEC_TRUE\"] = coords.skycoord.icrs.dec.to(\"deg\")\n",
    "\n",
    "        time_start, time_stop, time_ref = (gti.time_start, gti.time_stop, gti.time_ref)\n",
    "        time = temporal_model.sample_time(\n",
    "            n_events, time_start, time_stop, self.random_state\n",
    "        )\n",
    "        table[\"TIME\"] = u.Quantity(((time.mjd - time_ref.mjd) * u.day).to(u.s)).to(\"s\")\n",
    "        return table\n",
    "\n",
    "    def sample_sources(self, dataset):\n",
    "        \"\"\"Sample source model components.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.MapDataset`\n",
    "            Map dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list\n",
    "        \"\"\"\n",
    "        events_all = []\n",
    "\n",
    "        for idx, model in enumerate(dataset.models):\n",
    "            if isinstance(model, BackgroundModel):\n",
    "                continue\n",
    "\n",
    "            evaluator = dataset.evaluators.get(model.name)\n",
    "\n",
    "            evaluator = copy.deepcopy(evaluator)\n",
    "            evaluator.model.apply_irf[\"psf\"] = False\n",
    "            evaluator.model.apply_irf[\"edisp\"] = False\n",
    "            npred = evaluator.compute_npred()\n",
    "\n",
    "            temporal_model = ConstantTemporalModel()\n",
    "\n",
    "            table = self._sample_coord_time(npred, temporal_model, dataset.gti)\n",
    "            table[\"MC_ID\"] = idx + 1\n",
    "            events_all.append(EventList(table))\n",
    "\n",
    "        return EventList.stack(events_all)\n",
    "\n",
    "    def sample_background(self, dataset):\n",
    "        \"\"\"Sample background\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.MapDataset`\n",
    "            Map dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : `gammapy.data.EventList`\n",
    "            Background events\n",
    "        \"\"\"\n",
    "        background = dataset.background_model.evaluate()\n",
    "\n",
    "        temporal_model = ConstantTemporalModel()\n",
    "\n",
    "        table = self._sample_coord_time(background, temporal_model, dataset.gti)\n",
    "\n",
    "        table[\"MC_ID\"] = 0\n",
    "        table.rename_column(\"ENERGY_TRUE\", \"ENERGY\")\n",
    "        table.rename_column(\"RA_TRUE\", \"RA\")\n",
    "        table.rename_column(\"DEC_TRUE\", \"DEC\")\n",
    "\n",
    "        return EventList(table)\n",
    "\n",
    "    def sample_edisp(self, edisp_map, events):\n",
    "        \"\"\"Sample energy dispersion map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edisp_map : `~gammapy.cube.EDispMap`\n",
    "            Energy dispersion map\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list with the true energies\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list with reconstructed energy column.\n",
    "        \"\"\"\n",
    "        coord = MapCoord(\n",
    "            {\n",
    "                \"lon\": events.table[\"RA_TRUE\"].quantity,\n",
    "                \"lat\": events.table[\"DEC_TRUE\"].quantity,\n",
    "                \"energy_true\": events.table[\"ENERGY_TRUE\"].quantity,\n",
    "            },\n",
    "            frame=\"icrs\",\n",
    "        )\n",
    "\n",
    "        coords_reco = edisp_map.sample_coord(coord, self.random_state)\n",
    "        events.table[\"ENERGY\"] = coords_reco[\"energy\"]\n",
    "        return events\n",
    "\n",
    "    def sample_psf(self, psf_map, events):\n",
    "        \"\"\"Sample psf map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psf_map : `~gammapy.cube.PSFMap`\n",
    "            PSF map.\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list with reconstructed position columns.\n",
    "        \"\"\"\n",
    "        coord = MapCoord(\n",
    "            {\n",
    "                \"lon\": events.table[\"RA_TRUE\"].quantity,\n",
    "                \"lat\": events.table[\"DEC_TRUE\"].quantity,\n",
    "                \"energy_true\": events.table[\"ENERGY_TRUE\"].quantity,\n",
    "            },\n",
    "            frame=\"icrs\",\n",
    "        )\n",
    "\n",
    "        coords_reco = psf_map.sample_coord(coord, self.random_state)\n",
    "        events.table[\"RA\"] = coords_reco[\"lon\"] * u.deg\n",
    "        events.table[\"DEC\"] = coords_reco[\"lat\"] * u.deg\n",
    "        return events\n",
    "\n",
    "    @staticmethod\n",
    "    def event_list_meta(dataset, observation):\n",
    "        \"\"\"Event list meta info.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.MapDataset`\n",
    "            Map dataset.\n",
    "        observation : `~gammapy.data.Observation`\n",
    "            In memory observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        meta : dict\n",
    "            Meta dictionary.\n",
    "        \"\"\"\n",
    "        # See: https://gamma-astro-data-formats.readthedocs.io/en/latest/events/events.html#mandatory-header-keywords\n",
    "        meta = {}\n",
    "\n",
    "        meta[\"HDUCLAS1\"] = \"EVENTS\"\n",
    "        meta[\"EXTNAME\"] = 'EVENTS'\n",
    "        meta[\n",
    "            \"HDUDOC\"\n",
    "        ] = \"https://github.com/open-gamma-ray-astro/gamma-astro-data-formats\"\n",
    "        meta[\"HDUVER\"] = \"0.2\"\n",
    "        meta[\"HDUCLASS\"] = \"GADF\"\n",
    "\n",
    "        meta[\"OBS_ID\"] = observation.obs_id\n",
    "\n",
    "        meta[\"TSTART\"] = (\n",
    "            ((observation.tstart.mjd - dataset.gti.time_ref.mjd) * u.day).to(u.s).value\n",
    "        )\n",
    "        meta[\"TSTOP\"] = (\n",
    "            ((observation.tstop.mjd - dataset.gti.time_ref.mjd) * u.day).to(u.s).value\n",
    "        )\n",
    "        meta[\"ONTIME\"] = observation.observation_time_duration.to(\"s\").value\n",
    "        meta[\"LIVETIME\"] = observation.observation_live_time_duration.to(\"s\").value\n",
    "        meta[\"DEADC\"] = observation.observation_dead_time_fraction\n",
    "\n",
    "        meta[\"RA_PNT\"] = observation.pointing_radec.icrs.ra.deg\n",
    "        meta[\"DEC_PNT\"] = observation.pointing_radec.icrs.dec.deg\n",
    "\n",
    "        meta[\"EQUINOX\"] = \"J2000\"\n",
    "        meta[\"RADECSYS\"] = \"icrs\"\n",
    "        # TO DO: these keywords should be taken from the IRF of the dataset\n",
    "        meta[\"ORIGIN\"] = \"Gammapy\"\n",
    "        meta[\"TELESCOP\"] = \"\"\n",
    "        meta[\"INSTRUME\"] = \"\"\n",
    "        #\n",
    "        meta[\"CREATOR\"] = \"Gammapy {}\".format(gammapy.__version__)\n",
    "\n",
    "        #        TO COMPLETE\n",
    "        #        meta[\"OBSERVER\"] = \"\"\n",
    "        #        meta[\"CREATED\"] = \"\"\n",
    "        #        meta[\"OBJECT\"] = \"\"\n",
    "        #        meta[\"RA_OBJ\"] = \"\"\n",
    "        #        meta[\"DEC_OBJ\"] = \"\"\n",
    "        #        meta[\"OBS_MODE\"] = \"\"\n",
    "        #        meta[\"EV_CLASS\"] = \"\"\n",
    "        #        meta[\"TELAPSE\"] = \"\"\n",
    "        #\n",
    "        #        meta[\"MJDREFI\"] = int(dataset.gti.time_ref.mjd)\n",
    "        #        meta[\"MJDREFF\"] = dataset.gti.time_ref.mjd % 1\n",
    "        #        meta[\"TIMEUNIT\"] = \"s\"\n",
    "        #        meta[\"TIMESYS\"] = \"TT\"\n",
    "        #        meta[\"TIMEREF\"] = \"LOCAL\"\n",
    "        #        #         meta[\"DATE-OBS\"] = dataset.gti.time_start.isot[0][0:10]\n",
    "        #        #         meta[\"DATE-END\"] = dataset.gti.time_stop.isot[0][0:10]\n",
    "        #        meta[\"TIME-OBS\"] = dataset.gti.time_start.isot[0]\n",
    "        #        meta[\"TIME-END\"] = dataset.gti.time_stop.isot[0]\n",
    "        #\n",
    "\n",
    "        meta[\"GEOLON\"] = 0\n",
    "        meta[\"GEOLAT\"] = 0\n",
    "        meta[\"ALTITUDE\"] = 0\n",
    "\n",
    "        for idx, model in enumerate(dataset.models):\n",
    "            meta[\"MID{:05d}\".format(idx + 1)] = idx + 1\n",
    "            meta[\"MMN{:05d}\".format(idx + 1)] = model.name\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def run(self, dataset, observation=None):\n",
    "        \"\"\"Run the event sampler, applying IRF corrections.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.MapDataset`\n",
    "            Map dataset\n",
    "        observation : `~gammapy.data.Observation`\n",
    "            In memory observation.\n",
    "        edisp : Bool\n",
    "            It allows to include or exclude the Edisp in the simulation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        events : `~gammapy.data.EventList`\n",
    "            Event list.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataset.models[1]\n",
    "            events_src = self.sample_sources(dataset)\n",
    "\n",
    "            if dataset.psf:\n",
    "                events_src = self.sample_psf(dataset.psf, events_src)\n",
    "            else:\n",
    "                events_src.table[\"RA\"] = events_src.table[\"RA_TRUE\"]\n",
    "                events_src.table[\"DEC\"] = events_src.table[\"DEC_TRUE\"]\n",
    "\n",
    "            if dataset.edisp:\n",
    "                events_src = self.sample_edisp(dataset.edisp, events_src)\n",
    "            else:\n",
    "                events_src.table[\"ENERGY\"] = events_src.table[\"ENERGY_TRUE\"]\n",
    "\n",
    "            if dataset.background_model:\n",
    "                events_bkg = self.sample_background(dataset)\n",
    "                events = EventList.stack([events_bkg, events_src])\n",
    "            else:\n",
    "                events = events_src\n",
    "\n",
    "        except:\n",
    "            if dataset.background_model:\n",
    "                events_bkg = self.sample_background(dataset)\n",
    "                events = EventList.stack([events_bkg])\n",
    "\n",
    "        events.table[\"EVENT_ID\"] = np.arange(len(events.table))\n",
    "        events.table.meta = self.event_list_meta(dataset, observation)\n",
    "\n",
    "        return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path(\"../male.py\").parent\n",
    "\n",
    "AVAILABLE_MODELS = [\"background\"]\n",
    "\n",
    "DPI = 120\n",
    "\n",
    "# observation config\n",
    "IRF_FILE = \"$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\"\n",
    "#IRF_FILE = \"$GAMMAPY_DATA/cta-prod3b/caldb/data/cta/prod3b-v2/bcf/South_z20_50h/irf_file.fits\"\n",
    "\n",
    "POINTING = SkyCoord(0.0, 0.0, frame=\"galactic\", unit=\"deg\")\n",
    "LIVETIME = 1 * u.hr\n",
    "GTI_TABLE = GTI.create(start=0 * u.s, stop=LIVETIME.to(u.s))\n",
    "\n",
    "# dataset config\n",
    "ENERGY_AXIS = MapAxis.from_energy_bounds(\"0.1 TeV\", \"100 TeV\", nbin=10, per_decade=True)\n",
    "ENERGY_AXIS_TRUE = MapAxis.from_energy_bounds(\"0.03 TeV\", \"300 TeV\", nbin=20, per_decade=True, name=\"energy_true\")\n",
    "MIGRA_AXIS = MapAxis.from_bounds(0.5, 2, nbin=150, node_type=\"edges\", name=\"migra\")\n",
    "\n",
    "WCS_GEOM = WcsGeom.create(\n",
    "    skydir=POINTING, width=(4, 4), binsz=0.02, frame=\"galactic\", axes=[ENERGY_AXIS]\n",
    ")\n",
    "\n",
    "WCS_GEOM_fit = WcsGeom.create(\n",
    "    skydir=POINTING, width=(4, 4), binsz=0.1, frame=\"galactic\", axes=[ENERGY_AXIS]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_dataset(livetime):\n",
    "    filename = f\"data/dataset_{livetime.value:.0f}{livetime.unit}.fits.gz\"\n",
    "    return BASE_PATH / filename\n",
    "\n",
    "\n",
    "def get_filename_events(filename_dataset, filename_model, obs_id):\n",
    "    obs_id=int(obs_id)\n",
    "    model_str = filename_model.name.replace(filename_model.suffix, \"\")\n",
    "    filename_events = filename_dataset.name.replace(\"dataset\", \"events\")\n",
    "    filename_events = BASE_PATH / f\"data/models/{model_str}/\" / filename_events\n",
    "    filename_events = filename_events.name.replace(\".fits.gz\", f\"_{obs_id:04d}.fits.gz\")\n",
    "    path = BASE_PATH / f\"data/models/{model_str}/\" / filename_events\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_filename_best_fit_model(filename_model, obs_id, livetime):\n",
    "    obs_id=int(obs_id)\n",
    "    model_str = filename_model.name.replace(filename_model.suffix, \"\")\n",
    "    \n",
    "    path = BASE_PATH / f\"results/models/{model_str}/fit_{livetime.value:.0f}{livetime.unit}/covariance\"\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    path = BASE_PATH / f\"results/models/{model_str}/plots_{livetime.value:.0f}{livetime.unit}\"\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    filename = f\"results/models/{model_str}/fit_{livetime.value:.0f}{livetime.unit}/best-fit-model_{obs_id:04d}.yaml\"\n",
    "    return BASE_PATH / filename\n",
    "\n",
    "\n",
    "def get_filename_covariance(filename_best_fit_model):\n",
    "    filename = filename_best_fit_model.name\n",
    "    filename = filename.replace(\"best-fit-model\", \"covariance\")\n",
    "    filename = filename.replace(\".yaml\", \".txt\")\n",
    "    return filename_best_fit_model.parent / \"covariance\" / filename\n",
    "\n",
    "def read_best_fit_model(filename):\n",
    "    log.info(f\"Reading {filename}\")\n",
    "    model_best_fit = Models.read(filename)\n",
    "\n",
    "    path = get_filename_covariance(filename)\n",
    "    log.info(f\"Reading {path}\")\n",
    "    pars = model_best_fit.parameters\n",
    "    pars.covariance = np.loadtxt(str(path))\n",
    "\n",
    "    if model_best_fit[0].tag  == 'SkyDiffuseCube':\n",
    "        spectral_model_best_fit = model_best_fit[0]\n",
    "        covar = pars.get_subcovariance(spectral_model_best_fit.parameters)\n",
    "        spectral_model_best_fit.parameters.covariance = covar\n",
    "            \n",
    "#        spatial_model_best_fit = model_best_fit[0].spatial_model\n",
    "#        covar = pars.get_subcovariance(spatial_model_best_fit.parameters)\n",
    "#        spatial_model_best_fit.parameters.covariance = covar\n",
    "\n",
    "    else:\n",
    "        spectral_model_best_fit = model_best_fit[0].spectral_model\n",
    "        covar = pars.get_subcovariance(spectral_model_best_fit.parameters)\n",
    "        spectral_model_best_fit.parameters.covariance = covar\n",
    "        \n",
    "        spatial_model_best_fit = model_best_fit[0].spatial_model\n",
    "        covar = pars.get_subcovariance(spatial_model_best_fit.parameters)\n",
    "        spatial_model_best_fit.parameters.covariance = covar\n",
    "\n",
    "    return model_best_fit\n",
    "\n",
    "\n",
    "def parse_obs_ids(obs_ids_str, model):\n",
    "    if \":\" in obs_ids_str:\n",
    "        start, stop = obs_ids_str.split(\":\")\n",
    "        obs_ids = np.arange(int(start), int(stop))\n",
    "    elif \",\" in obs_ids_str:\n",
    "        obs_ids = [int(_) for _ in obs_ids_str.split(\",\")]\n",
    "    elif obs_ids_str == \"all\":\n",
    "        n_obs = len(list(BASE_PATH.glob(f\"data/models/{model}/events_*.fits.gz\")))\n",
    "        obs_ids = np.arange(n_obs)\n",
    "    else:\n",
    "         obs_ids = [int(obs_ids_str)]\n",
    "    return obs_ids\n",
    "\n",
    "def read_dataset(filename_dataset, filename_model, obs_id):\n",
    "    log.info(f\"Reading {filename_dataset}\")\n",
    "    dataset = MapDataset.read(filename_dataset)\n",
    "\n",
    "    filename_events = get_filename_events(filename_dataset, filename_model, obs_id)\n",
    "    log.info(f\"Reading {filename_events}\")\n",
    "    events = EventList.read(filename_events)\n",
    "\n",
    "    counts = Map.from_geom(WCS_GEOM)\n",
    "    counts.fill_events(events)\n",
    "    dataset.counts = counts\n",
    "    return dataset\n",
    "\n",
    "def read_dataset_for_fit(filename_dataset, filename_dataset_fit, filename_model, obs_id):\n",
    "    log.info(f\"Reading {filename_dataset_fit}\")\n",
    "    dataset = MapDataset.read(filename_dataset_fit)\n",
    "\n",
    "    filename_events = get_filename_events(filename_dataset, filename_model, obs_id)\n",
    "    log.info(f\"Reading {filename_events}\")\n",
    "    events = EventList.read(filename_events)\n",
    "\n",
    "    counts = Map.from_geom(WCS_GEOM_fit)\n",
    "    counts.fill_events(events)\n",
    "    dataset.counts = counts\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(filename_dataset):\n",
    "    \"\"\"Prepare dataset for a given skymodel.\"\"\"\n",
    "    log.info(f\"Reading {IRF_FILE}\")\n",
    "    irfs = load_cta_irfs(IRF_FILE)\n",
    "    observation = Observation.create(\n",
    "        obs_id=1001, pointing=POINTING, livetime=LIVETIME, irfs=irfs\n",
    "    )\n",
    "\n",
    "    empty = MapDataset.create(WCS_GEOM, energy_axis_true=ENERGY_AXIS_TRUE, migra_axis=MIGRA_AXIS)\n",
    "    maker = MapDatasetMaker(selection=[\"exposure\", \"background\"])\n",
    "    dataset = maker.run(empty, observation)\n",
    "\n",
    "    filename_dataset.parent.mkdir(exist_ok=True, parents=True)\n",
    "    log.info(f\"Writing {filename_dataset}\")\n",
    "    dataset.write(filename_dataset, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_fit(filename_dataset):\n",
    "    \"\"\"Prepare dataset for a given skymodel.\"\"\"\n",
    "    log.info(f\"Reading {IRF_FILE}\")\n",
    "    irfs = load_cta_irfs(IRF_FILE)\n",
    "    observation = Observation.create(\n",
    "        obs_id=1001, pointing=POINTING, livetime=LIVETIME, irfs=irfs\n",
    "    )\n",
    "\n",
    "    empty = MapDataset.create(WCS_GEOM_fit, energy_axis_true=ENERGY_AXIS_TRUE, migra_axis=MIGRA_AXIS)\n",
    "    maker = MapDatasetMaker(selection=[\"exposure\", \"background\"])\n",
    "    dataset = maker.run(empty, observation)\n",
    "\n",
    "    filename_dataset.parent.mkdir(exist_ok=True, parents=True)\n",
    "    log.info(f\"Writing {filename_dataset}\")\n",
    "    dataset.write(filename_dataset, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_events(filename_model, filename_dataset, nobs):\n",
    "    \"\"\"Simulate events of the bkg and dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_model : str\n",
    "        Filename of the model definition.\n",
    "    filename_dataset : str\n",
    "        Filename of the dataset to use for simulation.\n",
    "    nobs : int\n",
    "        Number of obervations to simulate.\n",
    "    \"\"\"\n",
    "    log.info(f\"Reading {IRF_FILE}\")\n",
    "    irfs = load_cta_irfs(IRF_FILE)\n",
    "\n",
    "    log.info(f\"Reading {filename_dataset}\")\n",
    "    dataset = MapDataset.read(filename_dataset)\n",
    "\n",
    "    sampler = MapDatasetEventSampler(random_state=0)\n",
    "\n",
    "    for obs_id in np.arange(nobs):\n",
    "        observation = Observation.create(\n",
    "            obs_id=obs_id, pointing=POINTING, livetime=LIVETIME, irfs=irfs\n",
    "        )\n",
    "\n",
    "        events = sampler.run(dataset, observation)\n",
    "\n",
    "        path = get_filename_events(filename_dataset, filename_model, obs_id)\n",
    "        log.info(f\"Writing {path}\")\n",
    "        path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        events.table.write(str(path), overwrite=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(filename_model, filename_dataset, obs_id, filename_dataset_fit=None):\n",
    "    \"\"\"Fit the events using a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename_model : str\n",
    "        Filename of the model definition.\n",
    "    filename_dataset : str\n",
    "        Filename of the dataset to use for simulation.\n",
    "    obs_id : int\n",
    "        Observation ID.\n",
    "    \"\"\"\n",
    "    if filename_dataset_fit is not None:\n",
    "        dataset = read_dataset_for_fit(filename_dataset, filename_dataset_fit, filename_model, obs_id)\n",
    "    else:\n",
    "        dataset = read_dataset(filename_dataset, filename_model, obs_id)\n",
    "\n",
    "    dataset.background_model.parameters[\"norm\"].frozen = False\n",
    "    dataset.background_model.parameters[\"tilt\"].frozen = False\n",
    "    \n",
    "    fit = Fit([dataset])\n",
    "\n",
    "#     result = fit.run(optimize_opts={\"print_level\": 1})\n",
    "    result = fit.run()\n",
    "\n",
    "    log.info(f\"Fit info: {result}\")\n",
    "    path = get_filename_best_fit_model(filename_model, obs_id, LIVETIME)\n",
    "    log.info(f\"Writing {path}\")\n",
    "    dataset.models.write(str(path), overwrite=True)\n",
    "    path = get_filename_covariance(path)\n",
    "\n",
    "    log.info(f\"Writing {path}\")\n",
    "\n",
    "    # TODO: exclude background parameters for now, as they are fixed anyway\n",
    "    covariance = result.parameters.get_subcovariance(dataset.models.parameters)\n",
    "    np.savetxt(path, covariance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gather(model_name, livetime):\n",
    "    rows = []\n",
    "\n",
    "    path = (BASE_PATH / f\"results/models/{model_name}/fit_{livetime.value:.0f}{livetime.unit}\")\n",
    "\n",
    "    for filename in path.glob(\"*.yaml\"):\n",
    "        yaml_str = Path(filename).read_text()\n",
    "        model_best_fit = yaml.safe_load(yaml_str)\n",
    "        path = get_filename_covariance(filename)\n",
    "        covariance = np.loadtxt(str(path))\n",
    "\n",
    "        row = {}\n",
    "\n",
    "        for i in np.arange(len(model_best_fit['components'][0]['parameters'])):\n",
    "            name=model_best_fit['components'][0]['parameters'][i]['name']\n",
    "            value=model_best_fit['components'][0]['parameters'][i]['value']\n",
    "            row[name] = value\n",
    "            row[name + \"_err\"] = covariance[i,i]**0.5\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    table = table_from_row_data(rows)\n",
    "    name = f\"fit-results-all_{livetime.value:.0f}{livetime.unit}\"\n",
    "    filename = BASE_PATH / f\"results/models/{model_name}/{name}.fits.gz\"\n",
    "    log.info(f\"Writing {filename}\")\n",
    "    table.write(str(filename), overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pull_distribution(model_name, livetime):\n",
    "    name = f\"fit-results-all_{livetime.value:.0f}{livetime.unit}\"\n",
    "    filename = BASE_PATH / f\"results/models/{model_name}/{name}.fits.gz\"\n",
    "    results = Table.read(str(filename))\n",
    "\n",
    "    dataset = MapDataset.read(filename_dataset)\n",
    "    dataset.background_model.parameters[\"tilt\"].frozen = False\n",
    "    model_ref = dataset.models\n",
    "    names = [name for name in results.colnames if \"err\" not in name]\n",
    "\n",
    "    plots = f\"plots_{livetime.value:.0f}{livetime.unit}\"\n",
    "    for name in names:\n",
    "        # TODO: report mean and stdev here as well\n",
    "        values = results[name]\n",
    "        values_err = results[name + \"_err\"]\n",
    "        par = model_ref.parameters[name]\n",
    "\n",
    "        if par.frozen:\n",
    "            log.info(f\"Skipping frozen parameter: {name}\")\n",
    "            continue\n",
    "\n",
    "        pull = (values - par.value) / values_err\n",
    "\n",
    "#        print(\"Number of fits beyond 5 sigmas: \",(np.where( (pull<-5) )))\n",
    "        plt.hist(pull, bins=21, density=True, range=(-5,5))\n",
    "        plt.xlim(-5, 5)\n",
    "        plt.xlabel(\"(value - value_true) / error\")\n",
    "        plt.ylabel(\"PDF\")\n",
    "        plt.title(f\"Pull distribution for {model_name}: {name} \")\n",
    "        filename = f\"results/models/{model_name}/{plots}/pull-distribution-{name}.png\"\n",
    "        save_figure(filename)\n",
    "\n",
    "\n",
    "def save_figure(filename):\n",
    "    path = BASE_PATH / filename\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    log.info(f\"Writing {path}\")\n",
    "    plt.savefig(path, dpi=DPI)\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n",
      "WARNING: AstropyDeprecationWarning: Specified hdu=EVENTS not found, reading in first available table (hdu=1) instead. This will result in an error in future versions! [astropy.io.fits.connect]\n"
     ]
    }
   ],
   "source": [
    "change_binning_fit_dataset=True\n",
    "\n",
    "model = 'background'\n",
    "models = [model]\n",
    "filename_model = BASE_PATH / f\"models/{model}.yaml\"\n",
    "filename_dataset = get_filename_dataset(LIVETIME)\n",
    "filename_dataset_fit = BASE_PATH / \"data/dataset_1h_fit.fits.gz\"\n",
    "\n",
    "obs_ids=1000\n",
    "\n",
    "prepare_dataset(filename_dataset)\n",
    "prepare_dataset_fit(filename_dataset_fit)\n",
    "\n",
    "for model in models:\n",
    "#     simulate_events(filename_model=filename_model, filename_dataset=filename_dataset, nobs=obs_ids)\n",
    "    obs_ids = f\"0:{obs_ids}\"\n",
    "    obs_ids = parse_obs_ids(obs_ids, model)\n",
    "    with multiprocessing.Pool(processes=4) as pool:\n",
    "        if change_binning_fit_dataset==False:\n",
    "            args = zip(repeat(filename_model), repeat(filename_dataset), obs_ids)\n",
    "        else:\n",
    "            args = zip(repeat(filename_model), repeat(filename_dataset), obs_ids, repeat(filename_dataset_fit))\n",
    "        results = pool.starmap(fit_model, args)\n",
    "\n",
    "    fit_gather(model,LIVETIME)\n",
    "    plot_pull_distribution(model, LIVETIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,   1.,   2.,   1.,   0.,   3.,   3.,   0.,   0.,   1.,\n",
       "          5.,   2.,   3.,   2.,   0.,   1.,   4.,   1.,   4.,   7.,   2.,\n",
       "          4.,   3.,   2.,   3.,   4.,   4.,   5.,   2.,   8.,   3.,   1.,\n",
       "          2.,   5.,   6.,   8.,   5.,   2.,  10.,   2.,   7.,   2.,   6.,\n",
       "          5.,   5.,   5.,  11.,   5.,   3.,   9.,   8.,   7.,   5.,   3.,\n",
       "         13.,  16.,   6.,  15.,   9.,  13.,  14.,  13.,   8.,   5.,   6.,\n",
       "         10.,  14.,  11.,  12.,  10.,  10.,   8.,   9.,  10.,  14.,  19.,\n",
       "         11.,   5.,  13.,   5.,  12.,  16.,  13.,  13.,  13.,   7.,  17.,\n",
       "         19.,  18.,  10.,  13.,  19.,  10.,  16.,  13.,   9.,  16.,  14.,\n",
       "         12.,  16.,  15.,  18.,   8.,  16.,  22.,  15.,  12.,  18.,  24.,\n",
       "         12.,  15.,  20.,  14.,  16.,  23.,  19.,  18.,  17.,  20.,  19.,\n",
       "         19.,  19.,  25.,  18.,  18.,  25.,  26.,  23.,  29.,  16.,  12.,\n",
       "         18.,  24.,  20.,  16.,  20.,  18.,  22.,  19.,  18.,  22.,  32.,\n",
       "         23.,  24.,  20.,  20.,  20.,  18.,  21.,  16.,  18.,  26.,  24.,\n",
       "         32.,  26.,  14.,  17.,  23.,  18.,  25.,  27.,  26.,  21.,  32.,\n",
       "         20.,  30.,  18.,  29.,  32.,  29.,  28.,  23.,  14.,  32.,  24.,\n",
       "         21.,  23.,  27.,  33.,  28.,  21.,  25.,  31.,  28.,  30.,  25.,\n",
       "         18.,  23.,  29.,  25.,  18.,  27.,  35.,  37.,  24.,  26.,  24.,\n",
       "         47.,  26.,  28.,  27.,  27.,  30.,  30.,  26.,  36.,  35.,  31.,\n",
       "         26.,  36.,  30.,  28.,  32.,  28.,  28.,  30.,  44.,  40.,  39.,\n",
       "         27.,  38.,  36.,  31.,  36.,  30.,  36.,  28.,  37.,  31.,  26.,\n",
       "         39.,  38.,  32.,  37.,  30.,  28.,  24.,  50.,  41.,  36.,  36.,\n",
       "         35.,  34.,  32.,  22.,  37.,  45.,  39.,  25.,  34.,  39.,  28.,\n",
       "         31.,  38.,  44.,  32.,  44.,  51.,  28.,  42.,  25.,  50.,  33.,\n",
       "         51.,  29.,  48.,  35.,  35.,  38.,  42.,  49.,  48.,  50.,  32.,\n",
       "         34.,  43.,  37.,  35.,  37.,  47.,  53.,  34.,  40.,  33.,  44.,\n",
       "         51.,  35.,  36.,  49.,  43.,  50.,  50.,  48.,  45.,  45.,  34.,\n",
       "         38.,  36.,  48.,  44.,  40.,  45.,  42.,  48.,  48.,  51.,  45.,\n",
       "         48.,  43.,  61.,  61.,  51.,  52.,  52.,  59.,  39.,  43.,  53.,\n",
       "         44.,  54.,  45.,  46.,  34.,  50.,  46.,  45.,  39.,  44.,  64.,\n",
       "         42.,  60.,  45.,  56.,  38.,  47.,  46.,  55.,  46.,  39.,  55.,\n",
       "         53.,  53.,  55.,  55.,  49.,  49.,  49.,  61.,  51.,  51.,  54.,\n",
       "         58.,  51.,  42.,  53.,  44.,  58.,  56.,  61.,  67.,  56.,  57.,\n",
       "         48.,  47.,  48.,  59.,  42.,  48.,  50.,  66.,  40.,  57.,  54.,\n",
       "         56.,  47.,  61.,  41.,  48.,  69.,  47.,  51.,  54.,  73.,  69.,\n",
       "         58.,  54.,  59.,  57.,  54.,  63.,  67.,  58.,  78.,  66.,  67.,\n",
       "         53.,  55.,  40.,  60.,  59.,  83.,  51.,  55.,  67.,  56.,  60.,\n",
       "         66.,  55.,  68.,  55.,  61.,  58.,  55.,  67.,  49.,  69.,  67.,\n",
       "         65.,  59.,  54.,  85.,  60.,  65.,  84.,  70.,  68.,  54.,  60.,\n",
       "         57.,  61.,  70.,  76.,  69.,  59.,  69.,  60.,  69.,  73.,  49.,\n",
       "         60.,  57.,  87.,  60.,  82.,  72.,  65.,  69.,  63.,  62.,  71.,\n",
       "         70.,  67.,  59.,  65.,  70.,  86.,  49.,  59.,  76.,  61.,  70.,\n",
       "         60.,  69.,  59.,  69.,  68.,  77.,  67.,  74.,  75.,  76.,  64.,\n",
       "         60.,  72.,  63.,  65.,  60.,  54.,  68.,  64.,  67.,  75.,  65.,\n",
       "         78.,  78.,  66.,  77.,  63.,  58.,  68.,  73.,  76.,  76.,  68.,\n",
       "         70.,  67.,  77.,  82.,  82.,  65.,  84.,  78.,  58.,  57.,  86.,\n",
       "         54.,  76.,  60.,  74.,  67.,  73.,  66.,  87.,  87.,  72.,  84.,\n",
       "         63.,  70.,  68.,  63.,  88.,  70.,  62.,  92.,  67.,  78.,  82.,\n",
       "         78.,  80.,  86.,  66.,  81.,  73.,  86.,  73.,  89.,  81.,  78.,\n",
       "         76.,  88.,  76.,  78.,  81.,  68.,  88.,  73.,  84.,  78.,  79.,\n",
       "         81.,  62.,  87.,  91.,  95.,  81.,  75.,  83.,  85.,  90.,  72.,\n",
       "         86.,  77.,  69.,  87.,  87.,  83.,  73.,  69.,  63.,  72.,  93.,\n",
       "         86.,  87.,  79.,  86.,  78.,  81.,  80.,  84.,  86.,  93.,  93.,\n",
       "         78.,  72.,  84.,  85.,  86.,  95.,  68., 102.,  76., 102., 100.,\n",
       "         87.,  95.,  85., 114.,  83.,  87.,  87.,  77.,  86.,  94.,  94.,\n",
       "         79.,  87.,  85., 103.,  81.,  91.,  92., 116.,  85., 106.,  84.,\n",
       "         93.,  82.,  83.,  92.,  80.,  83.,  86.,  90.,  84.,  95.,  86.,\n",
       "         91.,  88., 102.,  83., 103.,  83.,  80.,  85.,  94.,  80.,  89.,\n",
       "         92.,  77.,  85.,  83.,  91.,  94.,  84.,  93.,  82.,  84., 114.,\n",
       "         91.,  90., 104.,  76.,  87.,  80., 105., 100.,  87.,  97., 107.,\n",
       "         88., 107.,  94., 101.,  88.,  80.,  74.,  96.,  89.,  81.,  92.,\n",
       "         89., 104.,  88., 110., 100., 109., 102.,  98.,  96.,  98., 111.,\n",
       "        104.,  95., 107.,  88.,  84.,  99., 114.,  84.,  87., 102.,  89.,\n",
       "         90., 103.,  99.,  82.,  97., 100., 121., 100., 100., 117., 101.,\n",
       "         91.,  96., 115., 117.,  97.,  94.,  82.,  91.,  83.,  83.,  92.,\n",
       "         80.,  81.,  89.,  86.,  73.,  89.,  68.,  89.,  82.,  71.,  70.,\n",
       "         69.,  77.,  76.,  50.,  58.,  59.,  64.,  68.,  59.,  66.,  66.,\n",
       "         63.,  62.,  63.,  61.,  55.,  64.,  63.,  55.,  79.,  69.,  57.,\n",
       "         43.,  62.,  71.,  67.,  56.,  48.,  63.,  67.,  50.,  48.,  50.,\n",
       "         55.,  53.,  61.,  51.,  61.,  54.,  55.,  44.,  53.,  56.,  46.,\n",
       "         57.,  45.,  45.,  55.,  45.,  48.,  41.,  58.,  52.,  47.,  57.,\n",
       "         50.,  48.,  37.,  49.,  41.,  54.,  47.,  43.,  40.,  42.,  58.,\n",
       "         36.,  51.,  39.,  40.,  41.,  46.,  50.,  33.,  38.,  40.,  38.,\n",
       "         49.,  35.,  31.,  35.,  38.,  37.,  32.,  33.,  37.,  35.,  46.,\n",
       "         30.,  38.,  35.,  42.,  34.,  37.,  29.,  34.,  36.,  36.,  42.,\n",
       "         29.,  34.,  37.,  37.,  30.,  36.,  23.,  32.,  28.,  33.,  34.,\n",
       "         37.,  31.,  36.,  31.,  27.,  37.,  44.,  33.,  32.,  29.,  34.,\n",
       "         25.,  22.,  31.,  29.,  22.,  23.,  34.,  29.,  23.,  26.,  21.,\n",
       "         21.,  24.,  29.,  28.,  25.,  20.,  18.,  32.,  23.,  18.,  30.,\n",
       "         22.,  30.,  21.,  25.,  26.,  37.,  22.,  17.,  14.,  19.,  14.,\n",
       "         26.,  25.,  21.,  19.,  15.,  32.,  17.,  21.,  18.,  24.,  15.,\n",
       "         19.,  20.,  12.,  10.,  15.,  23.,  19.,  12.,  14.,  16.,  22.,\n",
       "         25.,  22.,  13.,  20.,  15.,  12.,  20.,  15.,  15.,  14.,  12.,\n",
       "         11.,  12.,  11.,   9.,  12.,  13.,  14.,  20.,  14.,  13.,   9.,\n",
       "          9.,  10.,  10.,  11.,  12.,   9.,  13.,  10.,   4.,  12.,  11.,\n",
       "         14.,   7.,  12.,   4.,  13.,   8.,  10.,   8.,  10.,   8.,   6.,\n",
       "          6.,   7.,   8.,  12.,   5.,   8.,   8.,   7.,   6.,   8.,  10.,\n",
       "         10.,   5.,   8.,   3.,   8.,   5.,   1.,   4.,   6.,   5.,   7.,\n",
       "          6.,   2.,   5.,   1.,   5.,   1.,   5.,   5.,   7.,   3.,   2.,\n",
       "          4.,   3.,   3.,   2.,   1.,   1.,   3.,   1.,   1.,   2.,   1.,\n",
       "          4.,   4.,   0.,   0.,   0.,   1.,   0.,   1.,   2.,   1.]),\n",
       " array([0.00580998, 0.00862586, 0.01144174, ..., 2.81605652, 2.8188724 ,\n",
       "        2.82168828]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = 'background'\n",
    "obsid=0\n",
    "filename_events = BASE_PATH / f\"data/models/{model}/events_{LIVETIME.value:.0f}{LIVETIME.unit}_{obsid:04d}.fits.gz\"\n",
    "events = EventList.read(filename_events)\n",
    "\n",
    "evt_pos = SkyCoord(events.table['RA'], events.table['DEC'], frame='icrs')\n",
    "sep = POINTING.separation(evt_pos).value\n",
    "\n",
    "plt.hist(sep, bins=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.74941648, 2.2281028 , 1.97197328, ..., 1.20249898, 0.5554091 ,\n",
       "       0.21984091])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
